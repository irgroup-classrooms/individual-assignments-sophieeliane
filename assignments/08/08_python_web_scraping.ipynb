{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS08 / OR92 Data Modeling: Python - Web Scraping\n",
    "\n",
    "Timo Breuer, Faculty of Information Science and Communication Studies, Institute of Information Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** Some contents are taken from https://carpentries-incubator.github.io/lc-webscraping/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basics of the Web, HTTP, and HTML\n",
    "---\n",
    "\n",
    "### What is the Web?\n",
    "\n",
    "- A collection of interconnected documents and resources accessed via the internet.\n",
    "- Web browsers (e.g., Chrome, Firefox, Safari) are used to navigate the web.\n",
    "\n",
    "### What is HTTP?\n",
    "\n",
    "**HTTP = HyperText Transfer Protocol:**\n",
    "- It is a protocol for transferring data between a client (browser) and a server.\n",
    "  \n",
    "**Key Features:**\n",
    "- Request-Response Model\n",
    "- Stateless (no memory of previous requests).\n",
    "  \n",
    "**HTTP Methods:**\n",
    "- `GET`: Retrieve data.\n",
    "- `POST`: Send data.\n",
    "- `PUT`: Update data.\n",
    "- `DELETE`: Remove data.\n",
    "\n",
    "Example Request:\n",
    "```\n",
    "GET /index.html HTTP/1.1\n",
    "Host: example.com\n",
    "```\n",
    "\n",
    "Example Response:\n",
    "```\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: text/html\n",
    "```\n",
    "\n",
    "### What is HTML?\n",
    "\n",
    "**HTML (HyperText Markup Language):**\n",
    "- Language for structuring web pages.\n",
    "- Defines content and layout.\n",
    "\n",
    "**HTML Elements:**\n",
    "- Exmaple tags: `<h1>`, `<p>`, `<a>`.\n",
    "- Example attributes: class, id, href.\n",
    "\n",
    "**Basic HTML Example:**\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to My Page</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <a href=\"https://example.com\">Click Here</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "\n",
    "### How Does a Web Page Load?\n",
    "\n",
    "**Step-by-Step Process:**\n",
    "1.\tBrowser sends an HTTP request to a server.\n",
    "2.\tServer processes the request and sends an HTTP response (usually HTML).\n",
    "3.\tBrowser renders the HTML, CSS, and JavaScript to display the page.\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78?type=png)](https://mermaid.live/edit#pako:eNp1UctuwjAQ_BXLJ5DSNDHkeeBQ-qBVKyGSU5WLG2-TCGKntlOgiH-vk0APVFhaybs7M7v2HHAuGOAYK_hqgedwX9FC0jrjyJyGSl3lVUO5RndSbBXI_40E5HdXHzon2M1sNtRj0-cMLdJ0iVbdDKXRCOzCttDTQ4puK85gZ5e63owHgYFm-Cel-MxVjeAK0GiRvr2iueAauB5fDv0jLak04A57ba9H0HmJNhVfA0MSlGhlDgqN5klioRcTzzUtQF1dq3_XJf3qPiuDBom28NEYVWzhGmRNK2a-_tCRMqxLqCHDsbkyKtcZzvjR4GirRbLnOY61bMHCUrRFeU7ahlF99uxcNK68C2HST7pRJgdWaSHfBp97u3sMjg94h-NgYpPImYau5weTkISuhfc4Jq4duST0AsfzfH_q-ORo4Z9e1bUdhxA_ckjku8HEifzjL6b9u78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What is Web Scraping?\n",
    "---\n",
    "\n",
    "**Definition:** Extracting data from websites programmatically.\n",
    "\n",
    "**Why scrape?**\n",
    "- Converts non-tabular or poorly structured data into usable formats like .csv files, spreadsheets, or database-compatible formats.\n",
    "- Access information not available via APIs.\n",
    "- Automate data collection for analysis.\n",
    "\n",
    "**Use Cases:**\n",
    "- Data Collection: Acquire specific data points from websites.\n",
    "- Data Archiving: Preserve online data for future reference.\n",
    "- Change Tracking: Monitor updates or changes to online information\n",
    "- Examples:\n",
    "  - Data for machine learning.\n",
    "  - Price comparison/Competitive Analysis: Businesses scrape competitor prices to adjust their own.\n",
    "  - Contact Scraping: Collecting personal information for marketing purposes.\n",
    "  - Academic Research/Trend Analysis: Creating datasets for text mining and analysis in scholarly projects.\n",
    "  - Data Journalism: Investigative journalists scrape data for stories, especially when not easily accessible.\n",
    "\n",
    "---\n",
    "## Tools for Web Scraping\n",
    "---\n",
    "\n",
    "**Languages:** Python (preferred), but also PHP or Ruby (i.e., other scripting languages).\n",
    "\n",
    "**Python Libraries:**\n",
    "- **Requests:** https://requests.readthedocs.io/en/latest/  \n",
    "- **Beautiful Soup:** https://beautiful-soup-4.readthedocs.io/\n",
    "- **Scrapy:** https://scrapy.org/\n",
    "- **Selenium:** https://selenium-python.readthedocs.io/ \n",
    "- **Parsel:** https://parsel.readthedocs.io/en/latest/\n",
    "\n",
    "**Other Tools:**\n",
    "- **Browser Developer Tools** for inspection (Shortcut: `F12`)\n",
    "- **Regex** for string patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A first example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a local copy of the `index.html`. (Later you can keep it in memory without writing it to the disk.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Specify the URL of the page you want to download\u001b[39;00m\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.scrapethissite.com/pages/simple/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Specify the URL of the page you want to download\n",
    "url = \"https://www.scrapethissite.com/pages/simple/\"\n",
    "\n",
    "# Make a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the content to an HTML file\n",
    "    with open(\"index.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Page saved as 'page.html'\")\n",
    "else:\n",
    "    print(f\"Failed to download page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the file and get familiar with the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an alternative you can also use the Developer Tools of your web browser!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# If the HTML is saved locally, you can use:\n",
    "with open(\"index.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Initialize an empty dictionary to store the country data\n",
    "countries_dict = {}\n",
    "\n",
    "# Find all div elements with class 'col-md-4 country'\n",
    "countries = soup.find_all('div', class_='col-md-4 country')\n",
    "\n",
    "# Iterate over each country div\n",
    "for country in countries:\n",
    "    # Extract country name\n",
    "    name = country.find('h3', class_='country-name').get_text(strip=True).split('\\n')[-1].strip()\n",
    "    \n",
    "    # Extract capital, population, and area\n",
    "    capital = country.find('span', class_='country-capital').get_text(strip=True)\n",
    "    population = country.find('span', class_='country-population').get_text(strip=True)\n",
    "    area = country.find('span', class_='country-area').get_text(strip=True)\n",
    "\n",
    "    # Store the data in the dictionary\n",
    "    countries_dict[name] = {\n",
    "        'Capital': capital,\n",
    "        'Population': population,\n",
    "        'Area (km^2)': area\n",
    "    }\n",
    "\n",
    "# Print the resulting dictionary\n",
    "for country, data in countries_dict.items():\n",
    "    print(f\"{country}: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Don’t break the web: Denial of Service attacks\n",
    "---\n",
    "\n",
    "**Web Scraping Basics:**\n",
    "- Web scraping involves repeatedly querying a website and accessing multiple pages.\n",
    "- Each request to a web server consumes its resources, potentially impacting other users.\n",
    "\n",
    "**Risks of Excessive Requests:**\n",
    "- Sending too many requests in a short time can overload a server.\n",
    "- Overloading may block legitimate users or crash the server.\n",
    "- Hackers exploit this technique intentionally in Denial of Service (DoS) attacks.\n",
    "\n",
    "**Server Protections Against Abuse:**\n",
    "- Servers monitor for excessive requests from a single IP address.\n",
    "- They may block or ban the source to prevent misuse.\n",
    "\n",
    "**Challenges for Scrapers:**\n",
    "- Legitimate web scrapers can inadvertently mimic DoS attacks.\n",
    "- This may result in being banned from the website.\n",
    "\n",
    "**Preventive Measures:**\n",
    "- Insert random delays between requests to the server.\n",
    "- Example: Scrapy includes built-in safeguards like random delays between requests to avoid overloading servers. By default, Scrapy limits the risk of resembling a DoS attack.\n",
    "\n",
    "**Best Practices for Developers:**\n",
    "- Limit the number of pages scraped during development and debugging.\n",
    "- Use Scrapy’s allowed_domains property to restrict scraping to specific domains.\n",
    "- Avoid scraping at full scale until the scraper is thoroughly tested.\n",
    "\n",
    "**In summary:** With proper precautions, the risk of causing trouble is minimal! (Learn to play by the rules)\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Don’t steal: Copyright and fair use\n",
    "--- \n",
    "\n",
    "**Legal Considerations of Web Scraping:**\n",
    "- Scraping may be illegal if a website’s terms and conditions explicitly prohibit copying its content.\n",
    "- Violating such terms could lead to legal trouble.\n",
    "  \n",
    "**General Acceptance:**\n",
    "- Web scraping is often tolerated if it does not disrupt the regular use of a website.\n",
    "- It is comparable to using a browser to access publicly available web content.\n",
    "\n",
    "**Public vs. Protected Data:**\n",
    "- Scraping publicly available data (not behind authentication or paywalls) is generally acceptable.\n",
    "- Problems arise when scraped data is redistributed or shared inappropriately.\n",
    "\n",
    "**Copyright Concerns:**\n",
    "- Republishing scraped content, especially without permission, can constitute copyright infringement.\n",
    "- Simply posting content from one site onto another as one’s own is illegal.\n",
    "\n",
    "**Fair Use Exceptions:**\n",
    "- Using scraped data in aggregate or derivative formats may qualify as “fair use.”\n",
    "- Avoid passing off scraped data as original, copying it verbatim, or monetizing it without permission.\n",
    "\n",
    "**Legal Variability:**\n",
    "- Copyright and data privacy laws vary by country; check the laws applicable to your location.\n",
    "- Example: In Australia, scraping and storing personal information (e.g., names, phone numbers, email addresses) can be illegal, even if publicly available.\n",
    "\n",
    "**Personal vs. Large-Scale Use:**\n",
    "- For personal use, following general scraping guidelines is usually sufficient.\n",
    "- For large-scale research or commercial projects, seek legal advice beforehand.\n",
    "\n",
    "**University Resources:**\n",
    "- Universities often have a copyright office to assist with legal aspects of data scraping projects.\n",
    "- The university library is a good starting point for guidance on copyright issues.\n",
    "\n",
    "**Best Practices:** Scrape responsibly, respect terms of use, and ensure compliance with copyright laws.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## robots.txt\n",
    "---\n",
    "\n",
    "**What is robots.txt?**\n",
    "- A file placed at the root of a website (example.com/robots.txt) to communicate with web crawlers and bots.\n",
    "- It specifies which parts of the website can or cannot be accessed by automated programs.\n",
    "\n",
    "**Purpose of robots.txt:**\n",
    "- Helps manage server load by restricting unnecessary crawling.\n",
    "- Protects sensitive or non-public sections of a site from being indexed.\n",
    "- Provides guidelines for ethical web scraping and crawling.\n",
    "\n",
    "**How robots.txt Works:**\n",
    "- Uses directives to instruct bots (e.g., User-agent: * applies to all bots).\n",
    "\n",
    "**Common directives include:**\n",
    "- `Disallow`: specifies paths that bots should not crawl.\n",
    "- `Allow`: permits access to specific paths.\n",
    "- `Sitemap`: points bots to the website’s sitemap for better indexing.\n",
    "\n",
    "**Limitations of robots.txt:**\n",
    "- It is advisory, not enforceable; malicious or non-compliant bots can ignore it.\n",
    "- Does not provide security; sensitive content should be protected by other means (e.g., authentication).\n",
    "\n",
    "**Best Practices for Web Scraping:**\n",
    "- Always check a website’s robots.txt file before scraping.\n",
    "- Respect the rules specified in the file to avoid violating ethical or legal standards.\n",
    "- Use tools or libraries (e.g., Python’s robotsparser) to parse and adhere to robots.txt. https://docs.python.org/3/library/urllib.robotparser.html\n",
    "\n",
    "**Locating robots.txt:**\n",
    "- Visit the URL https://[website-domain]/robots.txt to view its contents.\n",
    "- Note that not all websites have a robots.txt file.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Web scraping code of conduct\n",
    "---\n",
    "\n",
    "If you adhere to the following simple rules, you will probably be fine.\n",
    "\n",
    "1. **Ask nicely.** If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble.\n",
    "2. **Don’t download copies of documents that are clearly not public.** For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that.\n",
    "3. **Check your local legislation.** For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly avaialable web sites, can be illegal (e.g. in Australia).\n",
    "4. **Don’t share downloaded content illegally.** Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don’t hold the right to share is illegal.\n",
    "Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it.\n",
    "5. **Don’t break the Internet.** Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk.\n",
    "6. **Publish your own data in a reusable way.** Don’t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Video recommendations \n",
    "---\n",
    "\n",
    "[David Kriesel - SpiegelMining – Reverse Engineering von Spiegel-Online @ 33c3](https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/33c3-7912-spiegelmining_reverse_engineering_von_spiegel-online/oembed\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "[David Kriesel - BahnMining - Pünktlichkeit ist eine Zier @ 36C3](https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier)\n",
    "\n",
    "<iframe width=\"512\" height=\"288\" src=\"https://media.ccc.de/v/36c3-10652-bahnmining_-_punktlichkeit_ist_eine_zier/oembed\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "## Further reading\n",
    "---\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_journalism  \n",
    "https://en.wikipedia.org/wiki/Web_scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key points\n",
    "---\n",
    "\n",
    "- Web scraping is, in general, legal and won’t get you into trouble.\n",
    "\n",
    "- There are a few things to be careful about, notably don’t overwhelm a web server and don’t steal content.\n",
    "\n",
    "- Be nice. In doubt, ask.\n",
    "\n",
    "_Source:_ https://carpentries-incubator.github.io/lc-webscraping/05-conclusion/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lab assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and evaluating NHL team stats since 1990\n",
    "\n",
    "Open this site to browse through a database of NHL team stats since 1990:\n",
    "\n",
    "https://www.scrapethissite.com/pages/forms/\n",
    "\n",
    "Luckily, the data is already provided in tabular format. However, not all of the data is shown in a single page. Instead, you will have to paginate through the database. In the following, your task is to scrape all of the data and find a way to automatically scrape all of the pages and fetch the complete database. Then write everything into a CSV file. The columns names should correspond to those in the HTML table. Afterwards you will load the CSV file with pandas and give answers to the following two questions:\n",
    "\n",
    "- How made the most \"wins\" in 1990, 2000, and 2010?\n",
    "- How many teams participated in 1991, 2001, and 2011?\n",
    "\n",
    "**In summary:**\n",
    "1. Scrape the data (find a way to obtain all data from a pages programmatically)\n",
    "2. Store the entire data as CSV file\n",
    "3. Load it with pandas\n",
    "4. Give answers to the questions above by making use of the DataFrame methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the example above, we recommend to use **requests** and **beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page saved as 'page.html'\n"
     ]
    }
   ],
   "source": [
    "import requests as rs\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.scrapethissite.com/pages/forms/?page_num=1&per_page=1000\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Save the content to an HTML file\n",
    "    with open(\"page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Page saved as 'page.html'\")\n",
    "else:\n",
    "    print(f\"Failed to download page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich in 'extracted_data.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Lokale HTML-Datei laden\n",
    "with open(\"page.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# BeautifulSoup-Objekt erstellen\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Tabelle finden\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "# Überprüfen, ob die Tabelle existiert\n",
    "if not table:\n",
    "    print(\"Keine Tabelle in der HTML-Datei gefunden.\")\n",
    "    exit()\n",
    "\n",
    "# Tabellenzeilen extrahieren\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Spaltenüberschriften (Header) extrahieren\n",
    "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Datenzeilen extrahieren\n",
    "data = []\n",
    "for row in rows[1:]:  # Überspringe die Header-Zeile\n",
    "    columns = [col.text.strip() for col in row.find_all('td')]\n",
    "    if columns:\n",
    "        data.append(columns)\n",
    "\n",
    "# DataFrame erstellen\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# DataFrame in eine CSV-Datei speichern\n",
    "csv_filename = \"extracted_data.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Daten erfolgreich in '{csv_filename}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed the tasks, please commit this notebook with the solution to your GitHub repository in the directory `assignments/08/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Team Name  Year  Wins  Losses  OT Losses  Win %  Goals For (GF)  \\\n",
      "0       Boston Bruins  1990    44      24        NaN  0.550             299   \n",
      "1      Buffalo Sabres  1990    31      30        NaN  0.388             292   \n",
      "2      Calgary Flames  1990    46      26        NaN  0.575             344   \n",
      "3  Chicago Blackhawks  1990    49      23        NaN  0.613             284   \n",
      "4   Detroit Red Wings  1990    34      38        NaN  0.425             273   \n",
      "\n",
      "   Goals Against (GA)  + / -  \n",
      "0                 264     35  \n",
      "1                 278     14  \n",
      "2                 263     81  \n",
      "3                 211     73  \n",
      "4                 298    -25  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV-Datei laden\n",
    "df = pd.read_csv('extracted_data.csv')\n",
    "\n",
    "# Erste Zeilen anzeigen, um die Daten zu überprüfen\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Jahr 1990 gewann Chicago Blackhawks mit 49 Siegen\n",
      "Im Jahr 2000 gewann Colorado Avalanche mit 52 Siegen\n",
      "Im Jahr 2010 gewann Vancouver Canucks mit 54 Siegen\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV-Datei laden\n",
    "df = pd.read_csv('extracted_data.csv')\n",
    "\n",
    "# Bereinige Spaltennamen\n",
    "df.columns = df.columns.str.strip()  # Entfernt Leerzeichen\n",
    "df.columns = df.columns.str.lower()  # Wandelt alle Spaltennamen in Kleinbuchstaben\n",
    "\n",
    "# Die Jahre, die wir analysieren wollen\n",
    "years = [1990, 2000, 2010]\n",
    "\n",
    "# Ergebnisse speichern\n",
    "most_wins = {}\n",
    "\n",
    "for year in years:\n",
    "    # Filtere nach Jahr und sortiere nach 'wins'\n",
    "    year_data = df[df['year'] == year]\n",
    "    if not year_data.empty:\n",
    "        # Team mit den meisten Siegen\n",
    "        top_team = year_data.sort_values(by='wins', ascending=False).iloc[0]\n",
    "        most_wins[year] = {'Team': top_team['team name'], 'Wins': top_team['wins']}\n",
    "    else:\n",
    "        print(f\"Keine Daten für das Jahr {year} gefunden.\")\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for year, stats in most_wins.items():\n",
    "    print(f\"Im Jahr {year} gewann {stats['Team']} mit {stats['Wins']} Siegen\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Jahr 1991 haben 22 Teams teilgenommen\n",
      "Im Jahr 2001 haben 30 Teams teilgenommen\n",
      "Im Jahr 2011 haben 30 Teams teilgenommen\n"
     ]
    }
   ],
   "source": [
    "# Die Jahre, die wir analysieren wollen\n",
    "years_participation = [1991, 2001, 2011]\n",
    "\n",
    "# Ergebnisse speichern\n",
    "team_counts = {}\n",
    "\n",
    "for year in years_participation:\n",
    "    # Filtere nach Jahr\n",
    "    year_data = df[df['year'] == year]\n",
    "    if not year_data.empty:\n",
    "        # Zähle eindeutige Teams\n",
    "        team_counts[year] = year_data['team name'].nunique()\n",
    "    else:\n",
    "        print(f\"Keine Daten für das Jahr {year} gefunden.\")\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for year, count in team_counts.items():\n",
    "    print(f\"Im Jahr {year} haben {count} Teams teilgenommen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
